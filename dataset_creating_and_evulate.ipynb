{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a9ff16",
   "metadata": {},
   "source": [
    "Due to an unforeseeable errors in the notebook environment, I encountered errors when attempting to import the exact same libaries as here on another notebook. As a result, I have consolidated everything into this notebook. \n",
    "First, In order to create the necessary dataset, I ran the relevant code which subsequently stored the data in the designated data folder. \n",
    "Following this, I proceeded to run the evaluation using both BERT and BiLSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396b23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a816bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae06462",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "#### the data are stored in Data/ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177523a7",
   "metadata": {},
   "source": [
    "### Subject_Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "151234a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    }
   ],
   "source": [
    "test_examples = [ \"John loves Mary.\",\n",
    "                 \"Mary loves John.\",\n",
    "                 \"John hates Mary.\",\n",
    "                 \"Mary hates John.\",]\n",
    "#I followed this guide to generate my instances https://www.kaggle.com/code/juliusalphonso/filling-in-masked-words-with-roberta\n",
    "def test_generator(test_examples, mask_indices, outputfile):\n",
    "    unmask = pipeline('fill-mask')\n",
    "    unmask.tokenizer.mask_token\n",
    "\n",
    "    challange_sentences = []\n",
    "    for example in test_examples:\n",
    "        example_words = example.split()\n",
    "        for mask_index in mask_indices:\n",
    "            masked_example = example_words.copy()\n",
    "            masked_example[mask_index] = unmask.tokenizer.mask_token\n",
    "            masked_text = \" \".join(masked_example)\n",
    "            generated = unmask(masked_text)\n",
    "            for result in generated:\n",
    "                new_example = example_words.copy()\n",
    "                new_example[mask_index] = result['token_str']\n",
    "                challange_sentences.append(new_example)\n",
    "\n",
    "    extended_list = []\n",
    "    for sentence in challange_sentences:\n",
    "        verb = sentence[1].strip()\n",
    "        sentence[-1] = sentence[-1].strip()\n",
    "        # Add a period at the end of the sentence if it's missing\n",
    "        # Check if the last token ends with a period\n",
    "        if not sentence[-1].endswith('.'):\n",
    "            # Add a period to the last token\n",
    "            sentence[-1] = sentence[-1] + '.'\n",
    "        \n",
    "        \n",
    "        output_json = {\n",
    "            \"sentence\": \" \".join(sentence),\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"verb\": verb,\n",
    "                    \"tags\": [\"B-ARG0\", \"B-V\", \"B-ARG1\", \"O\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        extended_list.append(output_json)\n",
    "\n",
    "    with open(outputfile, 'w') as outfile:\n",
    "        json.dump(extended_list, outfile)\n",
    "\n",
    "\n",
    "unmask = create_pipeline()\n",
    "outputfile= 'Data/Subject_Switching.json'\n",
    "# Mask the 1st token\n",
    "mask_indices = [0,2]\n",
    "test_generator(test_examples, mask_indices, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2203a32",
   "metadata": {},
   "source": [
    "### disambiguous noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "6d34359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "test_examples = [\n",
    "    [\"A\", \"fight\", \"with\", \"spiders\", \"makes\", \"you\", \"brave\", \".\"],\n",
    "    [\"A\", \"walk\", \"with\", \"spiders\", \"makes\", \"you\", \"smart\", \".\"]\n",
    "]\n",
    "\n",
    "#I followed this guide to generate my instances https://www.kaggle.com/code/juliusalphonso/filling-in-masked-words-with-roberta\n",
    "\n",
    "\n",
    "\n",
    "def test_generator(test_examples, mask_indices, outputfile):\n",
    "    unmask = pipeline('fill-mask')\n",
    "    unmask.tokenizer.mask_token\n",
    "\n",
    "    challange_sentences = []\n",
    "    for example in test_examples:\n",
    "        for mask_index in mask_indices:\n",
    "            masked_example = example.copy()\n",
    "            masked_example[mask_index] = unmask.tokenizer.mask_token\n",
    "            masked_text = \" \".join(masked_example)\n",
    "            generated = unmask(masked_text)\n",
    "            for result in generated:\n",
    "                new_example = example.copy()\n",
    "                new_example[mask_index] = result['token_str']\n",
    "                challange_sentences.append(new_example)\n",
    "\n",
    "    extended_list = []\n",
    "    for sentence in challange_sentences:\n",
    "        verb = sentence[4].strip()\n",
    "        # Remove any extra space before the period\n",
    "        sentence[-1] = sentence[-1].strip()\n",
    "\n",
    "        # Add a period at the end of the sentence if it's missing\n",
    "        # Check if the last token ends with a period\n",
    "        if not sentence[-1].endswith('.'):\n",
    "            # Add a period to the last token\n",
    "            sentence[-1] = sentence[-1] + '.'\n",
    "        output_json = {\n",
    "            \"sentence\": \" \".join(sentence),\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"verb\": verb,\n",
    "                    \"tags\": [\"B-ARG0\", \"I-ARG0\", \"I-ARG0\", \"I-ARG0\", \"B-V\", \"B-ARG1\", \"I-ARG1\", \"O\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        extended_list.append(output_json)\n",
    "\n",
    "    with open(outputfile, 'w') as outfile:\n",
    "        json.dump(extended_list, outfile)\n",
    "\n",
    "\n",
    "unmask = create_pipeline()\n",
    "outputfile= 'Data/disambiguous_noun.json'\n",
    "# Mask the 1st token\n",
    "mask_indices = [1]\n",
    "test_generator(test_examples, mask_indices, outputfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698c14c",
   "metadata": {},
   "source": [
    "### passive_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "ce0235d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "test_examples = [\n",
    "    \"The cake was baked by the chef.\",\n",
    "    \"The message was delivered by the courier.\",\n",
    "    \"The computer was repaired by the technician.\",\n",
    "    \"The book was written by the author.\",\n",
    "    \"The painting was created by the artist.\"]\n",
    "\n",
    "def test_generator(test_examples, mask_indices, outputfile):\n",
    "    unmask = pipeline('fill-mask')\n",
    "    unmask.tokenizer.mask_token\n",
    "\n",
    "    challange_sentences = []\n",
    "    for example in test_examples:\n",
    "        example_words = example.split()\n",
    "        for mask_index in mask_indices:\n",
    "            masked_example = example_words.copy()\n",
    "            masked_example[mask_index] = unmask.tokenizer.mask_token\n",
    "            masked_text = \" \".join(masked_example)\n",
    "            generated = unmask(masked_text)\n",
    "            for result in generated:\n",
    "                new_example = example_words.copy()\n",
    "                new_example[mask_index] = result['token_str']\n",
    "                challange_sentences.append(new_example)\n",
    "\n",
    "    extended_list = []\n",
    "    for sentence in challange_sentences:\n",
    "        verb1 = sentence[2].strip()\n",
    "        verb2 = sentence[3].strip()\n",
    "\n",
    "        # Remove any extra space before the period\n",
    "        sentence[-1] = sentence[-1].strip()\n",
    "\n",
    "        # Add a period at the end of the sentence if it's missing\n",
    "        # Check if the last token ends with a period\n",
    "        if not sentence[-1].endswith('.'):\n",
    "            # Add a period to the last token\n",
    "            sentence[-1] = sentence[-1] + '.'\n",
    "        \n",
    "        output_json = {\n",
    "            \"sentence\": \" \".join(sentence),\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"verb\": verb1,\n",
    "                    \"tags\": ['O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O']\n",
    "                },\n",
    "                {\n",
    "                    \"verb\": verb2,\n",
    "                    \"tags\": ['B-ARG1', 'I-ARG1', 'O', 'B-V', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'O']\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        extended_list.append(output_json)\n",
    "\n",
    "    with open(outputfile, 'w') as outfile:\n",
    "        json.dump(extended_list, outfile, indent=4)\n",
    "\n",
    "unmask = pipeline('fill-mask')\n",
    "outputfile = 'Data/passive_voice.json'\n",
    "# Mask the 1st, 3rd, and 6th token\n",
    "mask_indices = [1, 3, 6]\n",
    "test_generator(test_examples, mask_indices, outputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5d96a6",
   "metadata": {},
   "source": [
    "### compund_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f6bde844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_examples = [ \"The  cat and dog chased the ball.\",\n",
    "                 \"The  mom and dad helped the child.\",\n",
    "                 \"The boy and  girl played the game.\",\n",
    "                 \"The sun and  rays lit the sky.\",\n",
    "                \"The chef and  crew made the dish.\"]\n",
    "#I followed this guide to generate my instances https://www.kaggle.com/code/juliusalphonso/filling-in-masked-words-with-roberta\n",
    "def test_generator(test_examples, mask_indices, outputfile):\n",
    "    unmask = pipeline('fill-mask')\n",
    "    unmask.tokenizer.mask_token\n",
    "\n",
    "    challange_sentences = []\n",
    "    for example in test_examples:\n",
    "        example_words = example.split()\n",
    "        for mask_index in mask_indices:\n",
    "            masked_example = example_words.copy()\n",
    "            masked_example[mask_index] = unmask.tokenizer.mask_token\n",
    "            masked_text = \" \".join(masked_example)\n",
    "            generated = unmask(masked_text)\n",
    "            for result in generated:\n",
    "                new_example = example_words.copy()\n",
    "                new_example[mask_index] = result['token_str']\n",
    "                challange_sentences.append(new_example)\n",
    "\n",
    "    extended_list = []\n",
    "    for sentence in challange_sentences:\n",
    "        verb = sentence[4].strip()\n",
    "        sentence[-1] = sentence[-1].strip()\n",
    "        # Add a period at the end of the sentence if it's missing\n",
    "        # Check if the last token ends with a period\n",
    "        if not sentence[-1].endswith('.'):\n",
    "            # Add a period to the last token\n",
    "            sentence[-1] = sentence[-1] + '.'\n",
    "        \n",
    "        \n",
    "        output_json = {\n",
    "            \"sentence\": \" \".join(sentence),\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"verb\": verb,\n",
    "                    \"tags\": [\"B-ARG0\", \"I-ARG0\", \"I-ARG0\", \"I-ARG0\", \"B-V\", \"B-ARG1\", \"I-ARG1\", \"O\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        extended_list.append(output_json)\n",
    "\n",
    "    with open(outputfile, 'w') as outfile:\n",
    "        json.dump(extended_list, outfile, indent=4)\n",
    "\n",
    "\n",
    "unmask = create_pipeline()\n",
    "outputfile= 'Data/compund_subjects.json'\n",
    "# Mask the 1st token\n",
    "mask_indices = [1,3]\n",
    "test_generator(test_examples, mask_indices, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ce2b5",
   "metadata": {},
   "source": [
    "### Dative Construction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a4e4a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n",
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_examples = [ \"Jessica taught her son the alphabet.\",\n",
    "                 \"Kevin brought his girlfriend some flowers.\",\n",
    "                 \"Hannah showed her friends a video.\",\n",
    "                 \"Mark bought his son a toy.\"]\n",
    "#I followed this guide to generate my instances https://www.kaggle.com/code/juliusalphonso/filling-in-masked-words-with-roberta\n",
    "def test_generator(test_examples, mask_indices, outputfile):\n",
    "    unmask = pipeline('fill-mask')\n",
    "    unmask.tokenizer.mask_token\n",
    "\n",
    "    challange_sentences = []\n",
    "    for example in test_examples:\n",
    "        example_words = example.split()\n",
    "        for mask_index in mask_indices:\n",
    "            masked_example = example_words.copy()\n",
    "            masked_example[mask_index] = unmask.tokenizer.mask_token\n",
    "            masked_text = \" \".join(masked_example)\n",
    "            generated = unmask(masked_text)\n",
    "            for result in generated:\n",
    "                new_example = example_words.copy()\n",
    "                new_example[mask_index] = result['token_str']\n",
    "                challange_sentences.append(new_example)\n",
    "\n",
    "    extended_list = []\n",
    "    for sentence in challange_sentences:\n",
    "        verb = sentence[1].strip()\n",
    "        sentence[-1] = sentence[-1].strip()\n",
    "        # Add a period at the end of the sentence if it's missing\n",
    "        # Check if the last token ends with a period\n",
    "        if not sentence[-1].endswith('.'):\n",
    "            # Add a period to the last token\n",
    "            sentence[-1] = sentence[-1] + '.'\n",
    "        \n",
    "        \n",
    "        output_json = {\n",
    "            \"sentence\": \" \".join(sentence),\n",
    "            \"labels\": [\n",
    "                {\n",
    "                    \"verb\": verb,\n",
    "                    \"tags\": [\"B-ARG0\", \"B-V\", \"B-ARG2\", \"I-ARG2\", \"B-ARG1\", \"I-ARG1\", \"O\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        extended_list.append(output_json)\n",
    "\n",
    "    with open(outputfile, 'w') as outfile:\n",
    "        json.dump(extended_list, outfile, indent=4)\n",
    "\n",
    "\n",
    "unmask = create_pipeline()\n",
    "outputfile= 'Data/dative.json'\n",
    "# Mask the 1st token\n",
    "mask_indices = [1]\n",
    "test_generator(test_examples, mask_indices, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659378c",
   "metadata": {},
   "source": [
    "### loading the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7c827d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9633a98453cc4d65991aac2213bfb4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\fapay\\AppData\\Local\\Temp\\tmplqcj38mt\\config.json as plain json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7904485326284270a9b3f9b40e7c9e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e7d90b5d564908bd28b0d2cd69d231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cde28c7a64462ca62b39f203ac27d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990fc408644346cfa21fa22326acb600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261409e02ca6453db0d313abcf76b30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\fapay\\AppData\\Roaming\\Python\\Python39\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.3.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf29cf504a4f4a1696c9d9364d1d0887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\fapay\\AppData\\Local\\Temp\\tmpe4r9l5fb\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "bert = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e2889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\fapay\\AppData\\Local\\Temp\\tmp7d2nt4j4\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "bilstm = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476adbf9",
   "metadata": {},
   "source": [
    "### loading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "aba2054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_test_dataset(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "passive_voice = load_test_dataset('Data/passive_voice.json')\n",
    "disambiguous_noun = load_test_dataset('Data/disambiguous_noun.json')\n",
    "dative  = load_test_dataset('Data/dative.json')\n",
    "compund_subjects = load_test_dataset('Data/compund_subjects.json')\n",
    "Subject_Switching = load_test_dataset('Data/Subject_Switching.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae59686",
   "metadata": {},
   "source": [
    "#### evulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d9a9228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictor, test_dataset):\n",
    "    total_items = 0\n",
    "    correct_items = 0\n",
    "    incorrect_items = 0\n",
    "    incorrect_predictions = []\n",
    "\n",
    "    for test_case in test_dataset:\n",
    "        sentence = test_case['sentence']\n",
    "        expected_outputs = test_case['labels']\n",
    "        predicted_output = predictor.predict(sentence)\n",
    "\n",
    "        if not expected_outputs or not predicted_output['verbs']:\n",
    "            continue  # Skip empty lists\n",
    "\n",
    "        matched_verbs = []\n",
    "        for expected in expected_outputs:\n",
    "            total_items += 1\n",
    "            found_match = False\n",
    "            for predicted in predicted_output['verbs']:\n",
    "                if expected['verb'] == predicted['verb'] and expected['tags'] == predicted['tags']:\n",
    "                    found_match = True\n",
    "                    matched_verbs.append(predicted)\n",
    "                    break\n",
    "\n",
    "            if found_match:\n",
    "                correct_items += 1\n",
    "            else:\n",
    "                incorrect_items += 1\n",
    "                incorrect_predictions.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"expected_output\": expected,\n",
    "                    \"predicted_output\": predicted_output['verbs']\n",
    "                })\n",
    "\n",
    "        unmatched_verbs = [verb for verb in predicted_output['verbs'] if verb not in matched_verbs]\n",
    "        for unmatched_verb in unmatched_verbs:\n",
    "            total_items += 1\n",
    "            incorrect_items += 1\n",
    "            incorrect_predictions.append({\n",
    "                \"sentence\": sentence,\n",
    "                \"expected_output\": None,\n",
    "                \"predicted_output\": unmatched_verb\n",
    "            })\n",
    "\n",
    "    error_rate = incorrect_items / total_items\n",
    "    accuracy = correct_items / total_items\n",
    "\n",
    "    return error_rate, accuracy, incorrect_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "10b8ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def present_results(incorrect_predictions):\n",
    "    data = []\n",
    "\n",
    "    for prediction in incorrect_predictions:\n",
    "        sentence = prediction['sentence']\n",
    "        expected_output = prediction['expected_output']\n",
    "        predicted_output = prediction['predicted_output']\n",
    "\n",
    "        if expected_output is not None:\n",
    "            correct_tags = expected_output['tags']\n",
    "        else:\n",
    "            correct_tags = None\n",
    "\n",
    "        incorrect_preds = [pred['tags'] for pred in predicted_output if isinstance(pred, dict) and pred['tags'] != correct_tags]\n",
    "\n",
    "        if incorrect_preds:\n",
    "            data.append({\n",
    "                'Sentence': sentence,\n",
    "                'Correct Tags': correct_tags,\n",
    "                'Predicted Tags': incorrect_preds[0]  # Only add the first incorrect prediction\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "d0d522ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passive_Voice\n",
    "pv_bert_error_rate, pv_bert_accuracy, pv_bert_incorrect = evaluate_model(bert, passive_voice)\n",
    "pv_biltsm_error_rate, pv_bilstm_accuracy, pv_bilstm_incorrect = evaluate_model(bilstm, passive_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4e1df958",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = present_results(pv_bert_incorrect)\n",
    "results_df.to_csv('result/passive_voice_bert.tsv', sep='\\t', index=False)\n",
    "results_df = present_results(pv_bilstm_incorrect)\n",
    "results_df.to_csv('result/passive_voice_bilstm.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "103c504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disambiguous_noun\n",
    "dn_bert_error_rate, dn_bert_accuracy, dn_bert_incorrect = evaluate_model(bert, disambiguous_noun)\n",
    "dn_biltsm_error_rate, dn_bilstm_accuracy, dn_bilstm_incorrect = evaluate_model(bilstm, disambiguous_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "953003d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = present_results(dn_bert_incorrect)\n",
    "results_df.to_csv('result/disambiguous_noun_bert.tsv', sep='\\t', index=False)\n",
    "results_df = present_results(dn_bilstm_incorrect)\n",
    "results_df.to_csv('result/disambiguous_noun_bilstm.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3ad45cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dative\n",
    "dative_bert_error_rate, dative_bert_accuracy, dative_bert_incorrect = evaluate_model(bert, dative)\n",
    "dative_biltsm_error_rate, dative_bilstm_accuracy, dative_bilstm_incorrect = evaluate_model(bilstm, dative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2ef4035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = present_results(dative_bert_incorrect)\n",
    "results_df.to_csv('result/dative_bert.tsv', sep='\\t', index=False)\n",
    "results_df = present_results(dative_bilstm_incorrect)\n",
    "results_df.to_csv('result/dative_bilstm.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "996dcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compund_subjects\n",
    "cs_bert_error_rate, cs_bert_accuracy, cs_bert_incorrect = evaluate_model(bert, compund_subjects)\n",
    "cs_biltsm_error_rate, cs_bilstm_accuracy, cs_bilstm_incorrect = evaluate_model(bilstm, compund_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7af36233",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = present_results(cs_bert_incorrect)\n",
    "results_df.to_csv('result/compund_subjects_bert.tsv', sep='\\t', index=False)\n",
    "results_df = present_results(cs_bilstm_incorrect)\n",
    "results_df.to_csv('result/compund_subjects_bilstm.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "0530460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subject_Switching\n",
    "ss_bert_error_rate, ss_bert_accuracy, ss_bert_incorrect = evaluate_model(bert, Subject_Switching)\n",
    "ss_biltsm_error_rate, ss_bilstm_accuracy, ss_bilstm_incorrect = evaluate_model(bilstm, Subject_Switching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "f86c7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = present_results(ss_bert_incorrect)\n",
    "results_df.to_csv('result/Subject_Switching_bert.tsv', sep='\\t', index=False)\n",
    "results_df = present_results(ss_bilstm_incorrect)\n",
    "results_df.to_csv('result/Subject_Switching_bilstm.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d0e63534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'Kevin  bought his girlfriend some flowers.',\n",
       "  'expected_output': {'verb': 'bought',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG2', 'I-ARG2', 'B-ARG1', 'I-ARG1', 'O']},\n",
       "  'predicted_output': [{'verb': 'bought',\n",
       "    'description': '[ARG0: Kevin] [V: bought] [ARG4: his girlfriend] [ARG1: some flowers] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG4', 'I-ARG4', 'B-ARG1', 'I-ARG1', 'O']}]},\n",
       " {'sentence': 'Kevin  bought his girlfriend some flowers.',\n",
       "  'expected_output': None,\n",
       "  'predicted_output': {'verb': 'bought',\n",
       "   'description': '[ARG0: Kevin] [V: bought] [ARG4: his girlfriend] [ARG1: some flowers] .',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG4', 'I-ARG4', 'B-ARG1', 'I-ARG1', 'O']}},\n",
       " {'sentence': 'Mark  bought his son a toy.',\n",
       "  'expected_output': {'verb': 'bought',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG2', 'I-ARG2', 'B-ARG1', 'I-ARG1', 'O']},\n",
       "  'predicted_output': [{'verb': 'bought',\n",
       "    'description': '[ARG0: Mark] [V: bought] [ARG4: his son] [ARG1: a toy] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG4', 'I-ARG4', 'B-ARG1', 'I-ARG1', 'O']}]},\n",
       " {'sentence': 'Mark  bought his son a toy.',\n",
       "  'expected_output': None,\n",
       "  'predicted_output': {'verb': 'bought',\n",
       "   'description': '[ARG0: Mark] [V: bought] [ARG4: his son] [ARG1: a toy] .',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG4', 'I-ARG4', 'B-ARG1', 'I-ARG1', 'O']}},\n",
       " {'sentence': 'Mark  buys his son a toy.',\n",
       "  'expected_output': {'verb': 'buys',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG2', 'I-ARG2', 'B-ARG1', 'I-ARG1', 'O']},\n",
       "  'predicted_output': [{'verb': 'buys',\n",
       "    'description': '[ARG0: Mark] [V: buys] [ARG4: his son] [ARG1: a toy] .',\n",
       "    'tags': ['B-ARG0', 'B-V', 'B-ARG4', 'I-ARG4', 'B-ARG1', 'I-ARG1', 'O']}]},\n",
       " {'sentence': 'Mark  buys his son a toy.',\n",
       "  'expected_output': None,\n",
       "  'predicted_output': {'verb': 'buys',\n",
       "   'description': '[ARG0: Mark] [V: buys] [ARG4: his son] [ARG1: a toy] .',\n",
       "   'tags': ['B-ARG0', 'B-V', 'B-ARG4', 'I-ARG4', 'B-ARG1', 'I-ARG1', 'O']}}]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### inspection of the result here, for instnace:\n",
    "dative_bert_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d789d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
